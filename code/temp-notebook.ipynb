{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "from utils import *\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from experiments import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tflearn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models_dict import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "x, y, mask = load_data_will_student_solve_next_problem(hoc_num=18, minlen=2, y_is_seq=True, with_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "mask = np.broadcast_to(mask, (mask.shape[0], mask.shape[1], y.shape[2]))\n",
    "print mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, mask_train, mask_test = train_test_split(x, y, mask, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85464, 10, 50)\n",
      "(85464, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "print x_train.shape\n",
    "print mask_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A model with the same model_id 'predict_next_prob_binary' already exists. \n",
      "No differences found. Yay! \n"
     ]
    }
   ],
   "source": [
    "model_id = \"predict_next_prob_binary\"\n",
    "n_timesteps = x_train.shape[1]\n",
    "n_hidden = 128\n",
    "n_inputdim = x_train.shape[2]\n",
    "n_classes = 2\n",
    "architecture = 'lstm_predict_binary'\n",
    "check_model_exists_or_create_new(model_id, n_timesteps, n_inputdim, n_hidden, n_classes, architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tensorboard_logs/predict_next_prob_binary/\n",
      "../checkpoints/predict_next_prob_binary/\n"
     ]
    }
   ],
   "source": [
    "tensorboard_dir = '../tensorboard_logs/' + model_id + '/'\n",
    "checkpoint_path = '../checkpoints/' + model_id + '/'\n",
    "print (tensorboard_dir)\n",
    "print (checkpoint_path)\n",
    "\n",
    "check_if_path_exists_or_create(tensorboard_dir)\n",
    "check_if_path_exists_or_create(checkpoint_path)\n",
    "    \n",
    "tensorboard_verbose = 2 \n",
    "max_checkpoints = 3\n",
    "clip_gradients = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print (y * mask)[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequential_categorical_crossentropy(y_pred, y_true, mask=None):\n",
    "    \"\"\" Categorical Crossentropy.\n",
    "\n",
    "    Computes cross entropy between y_pred (logits) and y_true (labels).\n",
    "    Arguments:\n",
    "        y_pred: `3D-Tensor`, Predicted values. shape (None, n_timesteps, )\n",
    "        y_true: `3D-Tensor`, Targets (labels), a probability distribution.\n",
    "        mask: `3D-Tensor`, masks out the padded timesteps. (since sequences are not the equal length)\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"Sequential_Crossentropy\"):\n",
    "        n_timesteps = y_pred.shape\n",
    "        y_pred = tf.reshape(y_pred, [-1, n_timesteps * n_classes], name='Reshape')\n",
    "        \n",
    "        \n",
    "        y_pred /= tf.reduce_sum(y_pred,\n",
    "                                reduction_indices=len(y_pred.get_shape())-1,\n",
    "                                keep_dims=True)\n",
    "        # manual computation of crossentropy\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.cast(_EPSILON, dtype=_FLOATX),\n",
    "                                  tf.cast(1.-_EPSILON, dtype=_FLOATX))\n",
    "        if mask:\n",
    "            seq_cross_entropy = - tf.reduce_sum(tf.mul(y_true, mask) * tf.log(y_pred),\n",
    "                                                reduction_indices=len(y_pred.get_shape()) - 1)\n",
    "        else:\n",
    "            seq_cross_entropy = - tf.reduce_sum(y_true * tf.log(y_pred),\n",
    "                                   reduction_indices=len(y_pred.get_shape())-1)\n",
    "        return tf.reduce_mean(seq_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6680  | total loss: \u001b[1m\u001b[32m14.01141\u001b[0m\u001b[0m\n",
      "| Optimizer | epoch: 010 | loss: 14.01141 - acc: 0.0216 | val_loss: 13.58568 - val_acc: 0.0180 -- iter: 85464/85464\n",
      "Training Step: 6680  | total loss: \u001b[1m\u001b[32m14.01141\u001b[0m\u001b[0m\n",
      "| Optimizer | epoch: 010 | loss: 14.01141 - acc: 0.0216 | val_loss: 13.58568 - val_acc: 0.0180 -- iter: 85464/85464\n",
      "--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/LSTM.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/LSTM_1.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(shape=(None, n_timesteps, n_inputdim), dtype=tf.float32, name=\"X\")\n",
    "    mask = tf.placeholder(shape=(None, n_timesteps, n_classes), dtype=tf.float32, name=\"mask\")\n",
    "    Y = tf.placeholder(shape=(None, n_timesteps, n_classes), dtype=tf.float32, name=\"Y\")\n",
    "\n",
    "    tf.add_to_collection(tf.GraphKeys.INPUTS, X)\n",
    "    tf.add_to_collection(tf.GraphKeys.LAYER_TENSOR + '/' + \"X\", X)\n",
    "\n",
    "    tf.add_to_collection(tf.GraphKeys.INPUTS, mask)\n",
    "    tf.add_to_collection(tf.GraphKeys.LAYER_TENSOR + '/' + \"mask\", mask)\n",
    "\n",
    "    if Y not in tf.get_collection(tf.GraphKeys.TARGETS):\n",
    "        tf.add_to_collection(tf.GraphKeys.TARGETS, Y)\n",
    "\n",
    "    net = tflearn.lstm(X, n_hidden, return_seq=True)\n",
    "    net = tflearn.lstm(net, n_classes, return_seq=True)\n",
    "    # outgoing tensor of shape [None, max_seq_len, 2]\n",
    "#     net = tf.reshape(net, [-1, n_timesteps * n_classes], name='Reshape')\n",
    "    # mask y_pred to exclude certain timesteps\n",
    "    \n",
    "    masked_y = tf.mul(Y, mask) # element-wise multiplication with broadcasting\n",
    "    \n",
    "    net = tf.reshape(net, [-1, n_timesteps * n_classes], name='reshape_pred')\n",
    "    masked_y = tf.reshape(masked_y, [-1, n_timesteps * n_classes], name='reshape_true_y')\n",
    "    \n",
    "    loss = tflearn.categorical_crossentropy(net, masked_y)\n",
    "\n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, masked_y))\n",
    "#     loss = sequential_categorical_crossentropy(net, Y, mask=mask)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    \n",
    "    # TODO: Reformulate the accuracy function\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(net, 1), tf.argmax(masked_y, 1)), tf.float32),\n",
    "        name='acc')\n",
    "\n",
    "    # Using TFLearn Trainer\n",
    "    # Define a training op (op for backprop, only need 1 in this model)\n",
    "    trainop = tflearn.TrainOp(loss=loss, optimizer=optimizer,\n",
    "                              metric=accuracy, batch_size=128)\n",
    "    \n",
    "    tf.add_to_collection(tf.GraphKeys.TRAIN_OPS, trainop)\n",
    "    \n",
    "\n",
    "    # Create Trainer, providing all training ops. Tensorboard logs stored\n",
    "    # in /tmp/tflearn_logs/. It is possible to change verbose level for more\n",
    "    # details logs about gradients, variables etc...\n",
    "    trainer = tflearn.Trainer(train_ops=trainop, \n",
    "                               clip_gradients=clip_gradients,\n",
    "                               tensorboard_dir=tensorboard_dir,\n",
    "                               tensorboard_verbose=tensorboard_verbose,\n",
    "                               checkpoint_path=checkpoint_path,\n",
    "                               max_checkpoints=max_checkpoints)\n",
    "\n",
    "    trainer.fit({X: x_train, mask: mask_train, Y: y_train}, \n",
    "                val_feed_dicts={X: x_test, mask: mask_test, Y: y_test},\n",
    "                n_epoch=10, show_metric=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Feed dict asks for variable named 'X' but no such variable is known to exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-2cb8f5f7d976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     model.fit({'X': x_train, 'mask': mask_train}, y_train,\n\u001b[1;32m     51\u001b[0m                 \u001b[0mvalidation_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmask_test\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 n_epoch=32, show_metric=True, snapshot_step=100, run_id=run_id)\n\u001b[0m",
      "\u001b[0;32m/Users/lisa1010/dev/dkt/venv/lib/python2.7/site-packages/tflearn/models/dnn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;31m# TODO: check memory impact for large data and multiple optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         feed_dict = feed_dict_builder(X_inputs, Y_targets, self.inputs,\n\u001b[0;32m--> 182\u001b[0;31m                                       self.targets)\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mfeed_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mval_feed_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/dev/dkt/venv/lib/python2.7/site-packages/tflearn/utils.pyc\u001b[0m in \u001b[0;36mfeed_dict_builder\u001b[0;34m(X, Y, net_inputs, net_targets)\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                         raise Exception(\"Feed dict asks for variable named '%s' but no \"\n\u001b[0;32m--> 300\u001b[0;31m                                         \"such variable is known to exist\" % key)\n\u001b[0m\u001b[1;32m    301\u001b[0m                     \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Feed dict asks for variable named 'X' but no such variable is known to exist"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(shape=(None, n_timesteps, n_inputdim), dtype=tf.float32, name=\"X\")\n",
    "    mask = tf.placeholder(shape=(None, n_timesteps, n_classes), dtype=tf.float32, name=\"mask\")\n",
    "    Y = tf.placeholder(shape=(None, n_timesteps, n_classes), dtype=tf.float32, name=\"Y\")\n",
    "\n",
    "    tf.add_to_collection(tf.GraphKeys.INPUTS, X)\n",
    "    tf.add_to_collection(tf.GraphKeys.LAYER_TENSOR + '/' + \"X\", X)\n",
    "\n",
    "    tf.add_to_collection(tf.GraphKeys.INPUTS, mask)\n",
    "    tf.add_to_collection(tf.GraphKeys.LAYER_TENSOR + '/' + \"mask\", mask)\n",
    "\n",
    "    if Y not in tf.get_collection(tf.GraphKeys.TARGETS):\n",
    "        tf.add_to_collection(tf.GraphKeys.TARGETS, Y)\n",
    "\n",
    "    net = tflearn.lstm(X, n_hidden, return_seq=True)\n",
    "    net = tflearn.lstm(net, n_classes, return_seq=True)\n",
    "    # outgoing tensor of shape [None, max_seq_len, 2]\n",
    "#     net = tf.reshape(net, [-1, n_timesteps * n_classes], name='Reshape')\n",
    "    # mask y_pred to exclude certain timesteps\n",
    "    \n",
    "    masked_y = tf.mul(Y, mask) # element-wise multiplication with broadcasting\n",
    "    \n",
    "    net = tf.reshape(net, [-1, n_timesteps * n_classes], name='reshape_pred')\n",
    "    masked_y = tf.reshape(masked_y, [-1, n_timesteps * n_classes], name='reshape_true_y')\n",
    "    \n",
    "    loss = tflearn.categorical_crossentropy(net, masked_y)\n",
    "\n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net, masked_y))\n",
    "#     loss = sequential_categorical_crossentropy(net, Y, mask=mask)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    \n",
    "    # TODO: Reformulate the accuracy function\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(net, 1), tf.argmax(masked_y, 1)), tf.float32),\n",
    "        name='acc')\n",
    "\n",
    "    # Using TFLearn Trainer\n",
    "    # Define a training op (op for backprop, only need 1 in this model)\n",
    "    trainop = tflearn.TrainOp(loss=loss, optimizer=optimizer,\n",
    "                              metric=accuracy, batch_size=128)\n",
    "    \n",
    "    tf.add_to_collection(tf.GraphKeys.TRAIN_OPS, trainop)\n",
    "    \n",
    "    \n",
    "    model = tflearn.DNN(net, tensorboard_verbose=2, tensorboard_dir=tensorboard_dir, \\\n",
    "                            checkpoint_path=checkpoint_path, max_checkpoints=3)\n",
    "    \n",
    "\n",
    "    date_time_string = datetime.datetime.now().strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "    run_id = \"{}_{}\".format(model_id, date_time_string)\n",
    "    model.fit({'X': x_train, 'mask': mask_train}, y_train,\n",
    "                validation_set=({'X': x_test, 'mask': mask_test}, y_test),\n",
    "                n_epoch=32, show_metric=True, snapshot_step=100, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
